{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error\n",
    "import numpy as np \n",
    "import json\n",
    "\n",
    "# Import Libraries and packages from Keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dropout\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(n_past, input_dim):\n",
    "    \"\"\"\n",
    "    LSTM model architecture construction.\n",
    "    \n",
    "    Args\n",
    "    ----------\n",
    "    n_past : Integer\n",
    "        Holds the number observations used for forecasting.\n",
    "    \n",
    "    input_dim : Integer\n",
    "        Holds the number of inputs passed to the LSTM network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Regressor\n",
    "        Keras LSTM model architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Layer List dimensions\n",
    "    layers = [50,50,50,50,1] \n",
    "        \n",
    "    # Initializing the RNN\n",
    "    regressor = Sequential()\n",
    "\n",
    "    # Adding fist LSTM layer and Drop out Regularization\n",
    "    regressor.add(LSTM(units=layers[0], return_sequences=True, input_shape=(n_past, input_dim)))\n",
    "    regressor.add(Dropout(.2))\n",
    "\n",
    "    # Part 3 - Adding more layers\n",
    "\n",
    "    # Adding 2nd layer with some drop out regularization\n",
    "    regressor.add(LSTM(units=layers[1], return_sequences=True))\n",
    "    regressor.add(Dropout(.2))\n",
    "\n",
    "    # Adding 3rd layer with some drop out regularization\n",
    "    regressor.add(LSTM(units=layers[2], return_sequences=True))\n",
    "    regressor.add(Dropout(.2))\n",
    "\n",
    "    # Adding 4th layer with some drop out regularization\n",
    "    regressor.add(GRU(units=layers[3], return_sequences=False))\n",
    "    regressor.add(Dropout(.2))\n",
    "\n",
    "    # Output layer\n",
    "    regressor.add(Dense(units=layers[4], activation='linear'))\n",
    "\n",
    "    # Compiling the RNN\n",
    "    regressor.compile(optimizer='rmsprop', loss=\"mse\")  # Can change loss to mean-squared-error if you require.\n",
    "\n",
    "    \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
    "    \"\"\"\n",
    "    Find the names of the csv files.\n",
    "    \n",
    "    Args\n",
    "    ----------\n",
    "    path_to_dir : String\n",
    "        Holds the path to the files directory.\n",
    "    \n",
    "    suffix : String\n",
    "        Holds the related suffix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Filenames\n",
    "        The names of the found files.\n",
    "    \"\"\"\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(train_data_list, feature_index_list):\n",
    "    \"\"\"\n",
    "    Extracts the features chosen for training.\n",
    "    \n",
    "    Args\n",
    "    ----------\n",
    "    train_data_list : List\n",
    "        Holds the columns from the dataset.\n",
    "    \n",
    "    feature_index_list : List\n",
    "        Holds the related suffix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col_names\n",
    "        Returns the chosen feature names.\n",
    "    \"\"\"\n",
    "    getVar = lambda searchList, ind: [searchList[i] for i in ind]\n",
    "    col_names = getVar(train_data_list, feature_index_list)\n",
    "    print('Chosen features:',  col_names)\n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, cols):\n",
    "    \"\"\"\n",
    "    Removes commas with csv files.\n",
    "    \n",
    "    Args\n",
    "    ----------\n",
    "    data : Dataframe\n",
    "        Holds the   the dataset.\n",
    "    \n",
    "    cols : List\n",
    "        Holds the column names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_set\n",
    "        Returns the dataset.\n",
    "    \"\"\"\n",
    "    #Preprocess data for training by removing all commas\n",
    "    data = data[cols].astype(str)\n",
    "    for i in cols:\n",
    "        for j in range(0,len(data)):\n",
    "            data[i][j] = data[i][j].replace(\",\",\"\")\n",
    "    data = data.astype(float)\n",
    "    data_set = data.as_matrix() # Using multiple predictors.\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data, lower_bound, upper_bound):\n",
    "    \"\"\"\n",
    "    Normalization using MinMax values.\n",
    "    \n",
    "    Args\n",
    "    ----------\n",
    "    data : Dataframe\n",
    "        Holds the   the dataset.\n",
    "    \n",
    "    lower_bound : Integer\n",
    "        Holds index of the first feature.\n",
    "    \n",
    "    upper_bound : Integer\n",
    "        Holds index of the last feature .\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    data_set\n",
    "        Returns the scaled dataset.\n",
    "    \"\"\"\n",
    "    # Feature Scaling\n",
    "    sc = MinMaxScaler(feature_range=(lower_bound, upper_bound))\n",
    "    data_scaled = sc.fit_transform(data)\n",
    "    return data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, x_lower_bound, x_upper_bound, y_index):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train and test sets, \n",
    "    taking into account the number of past days (Lag) and the number of days used for forecasting,\n",
    "    \n",
    "    Args\n",
    "    ----------\n",
    "    dataset : Dataframe\n",
    "        Holds the   the dataset.\n",
    "    \n",
    "    x_lower_bound : Integer\n",
    "        Holds index of the first indicator.\n",
    "    \n",
    "    x_upper_bound : Integer\n",
    "        Holds index of the last indicator.\n",
    "    \n",
    "    y_index : Integer\n",
    "        Holds index of the predicted value.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_values, y_values : Numpy Array\n",
    "        Holds the train and test sets.\n",
    "    \"\"\"\n",
    "    # Creating a data structure with 60 timesteps and 1 output\n",
    "    X_values = []\n",
    "    y_value = []\n",
    "\n",
    "    n_future = 20  # Number of days you want to predict into the future\n",
    "    n_past = 60  # Number of past days you want to use to predict the future\n",
    "\n",
    "    for i in range(n_past, len(dataset) - n_future + 1):\n",
    "        X_values.append(dataset[i - n_past:i, x_lower_bound:x_upper_bound])\n",
    "        y_value.append(dataset[i+n_future-1:i + n_future, y_index])\n",
    "\n",
    "    #X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    return np.array(X_values), np.array(y_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visuzalizaion(y, y_hat):\n",
    "    \"\"\"\n",
    "    Visualzes the performance of the of the trained LSTM architecture, using the actual and predicted values.\n",
    "    \n",
    "    Args\n",
    "    ----------\n",
    "    y : Array\n",
    "        Holds actual target values.\n",
    "    \n",
    "    y_hat : Array\n",
    "        Holds the predicted target values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Empty.\n",
    "    \"\"\"\n",
    "    hfm, = plt.plot(y_hat, 'r', label='predicted_consuption_level')\n",
    "    hfm2, = plt.plot(y,'b', label = 'actual_consuption_level')\n",
    "\n",
    "    plt.legend(handles=[hfm,hfm2])\n",
    "    plt.title('Predictions vs Actual Price')\n",
    "    plt.xlabel('Sample index')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.savefig('graph.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(regressor = None, data = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains the proposed model architecture using a number of train and evaluate it's performance using predictions from \n",
    "    and trend visualization of the test datasets. \n",
    "    \n",
    "    Args\n",
    "    ----------\n",
    "    regressor : Keras object\n",
    "        Holds model architecture.\n",
    "    \n",
    "    data : Dataframe\n",
    "        Holds the datasets.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Empty.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load datasets\n",
    "    filenames = find_csv_filenames(\"datasets/train_datasets\")\n",
    "    for name in filenames:\n",
    "        #Impor Training and Test datasets\n",
    "        dataset_train = pd.read_csv(\"datasets/train_datasets/\" + name)\n",
    "\n",
    "        dataset_test = pd.read_csv(\"datasets/test_datasets/\" + name)\n",
    "\n",
    "        # Build model\n",
    "        lag = 60\n",
    "        input_dim = 2\n",
    "        regressor = model(lag, input_dim)\n",
    "\n",
    "        # Get features\n",
    "        ls = list(dataset_train)\n",
    "        cols = get_features(ls,[48,49])\n",
    "\n",
    "        # Dataset Pre-processing\n",
    "        training_set = preprocess(dataset_train, cols)\n",
    "        test_set = preprocess(dataset_test, cols)\n",
    "\n",
    "        # Feature Scaling train\n",
    "        sc_predict_train = MinMaxScaler(feature_range=(0,1)) \n",
    "        sc_predict_train.fit_transform(training_set[:,0:1])\n",
    "\n",
    "        # Feature Scaling test\n",
    "        sc_predict_test = MinMaxScaler(feature_range=(0,1)) \n",
    "        sc_predict_test.fit_transform(test_set[:,0:1])\n",
    "\n",
    "\n",
    "\n",
    "        training_set_scaled = scale(training_set, 0,2)       \n",
    "        test_set_scaled = scale(test_set, 0, 2)\n",
    "\n",
    "        # Train, test split\n",
    "        X_train, y_train = train_test_split(training_set_scaled, 0, 2, 0)\n",
    "        X_test, y_test = train_test_split(test_set_scaled, 0, 2, 0)\n",
    "\n",
    "\n",
    "        # Fitting RNN to training set using Keras Callbacks. Read Keras callbacks docs for more info. \n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=50, verbose=1)\n",
    "        rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "        mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "        tb = TensorBoard('logs')\n",
    "\n",
    "        history = regressor.fit(X_train, y_train, shuffle=True, epochs=100,\n",
    "                                callbacks=[es, rlr,mcp, tb], validation_split=0.2, verbose=1, batch_size=10)\n",
    "\n",
    "        # Prediction results\n",
    "        predictions_train = regressor.predict(X_train)\n",
    "        predictions_test = regressor.predict(X_test)\n",
    "\n",
    "        # Inverse train scaling transformation\n",
    "        predictions_plot_train = sc_predict_train.inverse_transform(predictions_train[0:-60])\n",
    "        actual_plot_train = sc_predict_train.inverse_transform(y_train[60:-1])\n",
    "\n",
    "        # Inverse test scaling transformation\n",
    "        predictions_plot_test = sc_predict_test.inverse_transform(predictions_test[0:-60])\n",
    "        actual_plot_test = sc_predict_test.inverse_transform(y_test[60:-1])\n",
    "\n",
    "        # Visualize trends\n",
    "        visuzalizaion(actual_plot_train,predictions_plot_train)\n",
    "        visuzalizaion(actual_plot_test,predictions_plot_test)\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen features: ['eload', 'Temp']\n",
      "Train on 6915 samples, validate on 1729 samples\n",
      "Epoch 1/100\n",
      "6915/6915 [==============================] - 70s 10ms/step - loss: 0.0353 - val_loss: 0.0135\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01347, saving model to weights.h5\n",
      "Epoch 2/100\n",
      "6915/6915 [==============================] - 83s 12ms/step - loss: 0.0193 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01347 to 0.00528, saving model to weights.h5\n",
      "Epoch 3/100\n",
      "6915/6915 [==============================] - 60s 9ms/step - loss: 0.0158 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/100\n",
      "6915/6915 [==============================] - 63s 9ms/step - loss: 0.0147 - val_loss: 0.0116\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/100\n",
      "6915/6915 [==============================] - 66s 9ms/step - loss: 0.0138 - val_loss: 0.0051\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00528 to 0.00509, saving model to weights.h5\n",
      "Epoch 6/100\n",
      "6915/6915 [==============================] - 65s 9ms/step - loss: 0.0133 - val_loss: 0.0064\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/100\n",
      "6915/6915 [==============================] - 69s 10ms/step - loss: 0.0128 - val_loss: 0.0111\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/100\n",
      "6915/6915 [==============================] - 68s 10ms/step - loss: 0.0120 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/100\n",
      "6915/6915 [==============================] - 65s 9ms/step - loss: 0.0117 - val_loss: 0.0098\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/100\n",
      "6915/6915 [==============================] - 66s 10ms/step - loss: 0.0114 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/100\n",
      "6915/6915 [==============================] - 70s 10ms/step - loss: 0.0108 - val_loss: 0.0104\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/100\n",
      "6915/6915 [==============================] - 69s 10ms/step - loss: 0.0099 - val_loss: 0.0124\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/100\n",
      "6915/6915 [==============================] - 68s 10ms/step - loss: 0.0097 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/100\n",
      "6915/6915 [==============================] - 69s 10ms/step - loss: 0.0096 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/100\n",
      "6915/6915 [==============================] - 67s 10ms/step - loss: 0.0095 - val_loss: 0.0134\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/100\n",
      "6915/6915 [==============================] - 69s 10ms/step - loss: 0.0093 - val_loss: 0.0088\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/100\n",
      "6915/6915 [==============================] - 73s 11ms/step - loss: 0.0089 - val_loss: 0.0093\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/100\n",
      "6915/6915 [==============================] - 69s 10ms/step - loss: 0.0085 - val_loss: 0.0079\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/100\n",
      "6915/6915 [==============================] - 69s 10ms/step - loss: 0.0085 - val_loss: 0.0144\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/100\n",
      "6915/6915 [==============================] - 69s 10ms/step - loss: 0.0085 - val_loss: 0.0092\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/100\n",
      "6915/6915 [==============================] - 69s 10ms/step - loss: 0.0085 - val_loss: 0.0098\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/100\n",
      "6915/6915 [==============================] - 70s 10ms/step - loss: 0.0081 - val_loss: 0.0114\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/100\n",
      "6915/6915 [==============================] - 70s 10ms/step - loss: 0.0079 - val_loss: 0.0108\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/100\n",
      "6915/6915 [==============================] - 76s 11ms/step - loss: 0.0080 - val_loss: 0.0098\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/100\n",
      " 820/6915 [==>...........................] - ETA: 1:14 - loss: 0.0074"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6a26178c02d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-439de7e9b164>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(regressor, data)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         history = regressor.fit(X_train, y_train, shuffle=True, epochs=100,\n\u001b[1;32m---> 48\u001b[1;33m                                 callbacks=[es, rlr,mcp, tb], validation_split=0.2, verbose=1, batch_size=10)\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Prediction results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_model()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
